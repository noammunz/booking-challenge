{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AutoModel\n",
    "from transformers import get_cosine_schedule_with_warmup\n",
    "import lightgbm as lgb\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook utilizes the embedding model we trained to embed the entire training dataset. It sorts the data by accommodation and then creates hard negatives by shifting the data within the same accommodation group. We then train a LightGBM model on this processed data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_users_features = pd.read_parquet('train_users_features.parquet')\n",
    "train_reviews_features = pd.read_parquet('train_reviews_features.parquet')\n",
    "train_matches = pd.read_csv('train_matches.csv')\n",
    "\n",
    "train_users_tokens = pd.read_parquet('train_users_tokens.parquet')\n",
    "train_reviews_tokens = pd.read_parquet('train_reviews_tokens.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function and classes for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_subsets(users_features, users_tokens, reviews_features, reviews_tokens, matches, total_parts, part):\n",
    "    \"\"\"\n",
    "    Create subsets of the data for training and evaluation to fit into memory\n",
    "    Args:\n",
    "        users_features: DataFrame with user features\n",
    "        users_tokens: DataFrame with user tokens\n",
    "        reviews_features: DataFrame with review features\n",
    "        reviews_tokens: DataFrame with review tokens\n",
    "        matches: DataFrame with matches\n",
    "        total_parts: total number of parts to split the data into\n",
    "        part: part to select\n",
    "    Returns:\n",
    "        Tuple of DataFrames with user features, user tokens, review features, review tokens, and matches\n",
    "    \"\"\"\n",
    "    accommodation_ids = matches['accommodation_id'].unique().tolist()\n",
    "    \n",
    "    accommodation_ids = sorted(accommodation_ids)\n",
    "    \n",
    "    subset_size = len(accommodation_ids) // total_parts\n",
    "    remainder = len(accommodation_ids) % total_parts\n",
    "    \n",
    "    start_idx = part * subset_size + min(part, remainder)\n",
    "    end_idx = (part + 1) * subset_size + min(part + 1, remainder)\n",
    "\n",
    "    selected_accommodation_ids = accommodation_ids[start_idx:end_idx]\n",
    "    \n",
    "    users_features = users_features[users_features['accommodation_id'].isin(selected_accommodation_ids)]\n",
    "    reviews_features = reviews_features[reviews_features['accommodation_id'].isin(selected_accommodation_ids)]\n",
    "    users_tokens = users_tokens[users_tokens['accommodation_id'].isin(selected_accommodation_ids)]\n",
    "    reviews_tokens = reviews_tokens[reviews_tokens['accommodation_id'].isin(selected_accommodation_ids)]\n",
    "    \n",
    "    matches_reduced = matches[matches['accommodation_id'].isin(selected_accommodation_ids)]\n",
    "    \n",
    "    return users_features, users_tokens, reviews_features, reviews_tokens, matches_reduced\n",
    "\n",
    "class TwoTowersNetwork(nn.Module):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        model_id (str): Model identifier.\n",
    "    \"\"\"\n",
    "    def __init__(self, model_id):\n",
    "        super().__init__()\n",
    "        self.bert1 = AutoModel.from_pretrained(model_id)\n",
    "        self.bert2 = AutoModel.from_pretrained(model_id)\n",
    "        \n",
    "    def mean_pooling(self, model_output, attention_mask):\n",
    "        token_embeddings = model_output[0]\n",
    "        mask = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "        return torch.sum(token_embeddings * mask, 1) / torch.clamp(mask.sum(1), min=1e-9)\n",
    "\n",
    "    def forward(self, user_ids, user_mask, review_ids, review_mask):\n",
    "        context_output = self.bert1(user_ids, attention_mask=user_mask)\n",
    "        review_output = self.bert2(review_ids, attention_mask=review_mask)\n",
    "        \n",
    "        context_embed = self.mean_pooling(context_output, user_mask)\n",
    "        review_embed = self.mean_pooling(review_output, review_mask)\n",
    "        \n",
    "        return context_embed, review_embed\n",
    "\n",
    "def pad_sequence(sequences, batch_first=False, padding_value=0):\n",
    "    \"\"\"\n",
    "    Pad a list of variable length sequences with padding_value.\n",
    "    \"\"\"\n",
    "    max_len = max(len(seq) for seq in sequences)\n",
    "    padded_sequences = []\n",
    "\n",
    "    for seq in sequences:\n",
    "        padding = torch.full((max_len - len(seq),), padding_value, dtype=seq.dtype)\n",
    "        padded_sequences.append(torch.cat((seq, padding)))\n",
    "\n",
    "    if batch_first:\n",
    "        return torch.stack(padded_sequences)\n",
    "    else:\n",
    "        return torch.stack(padded_sequences).transpose(0, 1)\n",
    "\n",
    "def encode_pairwise(train_users_tokens, train_reviews_tokens, model, batch_size=64, device='cuda'):\n",
    "    \"\"\"\n",
    "    Encode user and review tokens using the model.\n",
    "    Args:\n",
    "        train_users_tokens (DataFrame): DataFrame with user tokens\n",
    "        train_reviews_tokens (DataFrame): DataFrame with review tokens\n",
    "        model (nn.Module): Model to use for encoding\n",
    "        batch_size (int): Batch size\n",
    "        device (str): Device to use\n",
    "    Returns:\n",
    "        Tuple of DataFrames with user and review embeddings\n",
    "    \"\"\"\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    user_embeddings, review_embeddings = [], []\n",
    "\n",
    "    for i in range(0, len(train_users_tokens), batch_size):\n",
    "        user_batch = train_users_tokens.iloc[i:i+batch_size]\n",
    "        review_batch = train_reviews_tokens.iloc[i:i+batch_size]\n",
    "        \n",
    "        user_ids = pad_sequence([torch.tensor(x) for x in user_batch['input_ids']], batch_first=True).to(device)\n",
    "        user_masks = pad_sequence([torch.tensor(x) for x in user_batch['attention_mask']], batch_first=True).to(device)\n",
    "        review_ids = pad_sequence([torch.tensor(x) for x in review_batch['input_ids']], batch_first=True).to(device)\n",
    "        review_masks = pad_sequence([torch.tensor(x) for x in review_batch['attention_mask']], batch_first=True).to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            user_embed, review_embed = model(user_ids, user_masks, review_ids, review_masks)\n",
    "            user_embeddings.extend(user_embed.cpu().numpy())\n",
    "            review_embeddings.extend(review_embed.cpu().numpy())\n",
    "        \n",
    "        if (i // batch_size) % 500 == 0:\n",
    "            print(f'Processed {i}/{len(train_users_tokens)} samples', flush=True)\n",
    "    print('', flush=True)\n",
    "\n",
    "    user_embeddings_df = pd.DataFrame(user_embeddings, columns=[f'user_embedding_{i}' for i in range(user_embed.size(1))])\n",
    "    review_embeddings_df = pd.DataFrame(review_embeddings, columns=[f'review_embedding_{i}' for i in range(review_embed.size(1))])\n",
    "\n",
    "    return user_embeddings_df, review_embeddings_df\n",
    "\n",
    "def embed_data(users_tokens, reviews_tokens, model_path, batch_size=64, device='cuda'):\n",
    "    \"\"\"\n",
    "    Encode user and review tokens using the model.\n",
    "    Args:\n",
    "        users_tokens (DataFrame): DataFrame with user tokens\n",
    "        reviews_tokens (DataFrame): DataFrame with review tokens\n",
    "        model_path (str): Path to the model\n",
    "        batch_size (int): Batch size\n",
    "        device (str): Device to use\n",
    "    Returns:\n",
    "        Tuple of DataFrames with user and review embeddings\n",
    "    \"\"\"\n",
    "    model = TwoTowersNetwork(\"sentence-transformers/all-MiniLM-L12-v2\").to(device)\n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "\n",
    "    user_embeddings, review_embeddings = encode_pairwise(users_tokens, reviews_tokens, model, batch_size=batch_size, device=device)\n",
    "\n",
    "    \n",
    "    return user_embeddings, review_embeddings\n",
    "\n",
    "def generate_negative_samples_shift(df, split_idx):\n",
    "    \"\"\"\n",
    "    Generate negative samples by shifting the right part of the dataframe.\n",
    "    Args:\n",
    "        df (DataFrame): DataFrame with positive samples\n",
    "        split_idx (int): Index to split the dataframe\n",
    "    Returns:\n",
    "        DataFrame with negative samples\n",
    "    \"\"\"\n",
    "    df_sorted = df.sort_values('accommodation_id').reset_index(drop=True)\n",
    "    \n",
    "    left_part = df_sorted.iloc[:, :split_idx]\n",
    "    right_part = df_sorted.iloc[:, split_idx:]\n",
    "    \n",
    "    right_part_shifted = pd.concat([right_part.iloc[-1:], right_part.iloc[:-1]]).reset_index(drop=True)\n",
    "    \n",
    "    df_negative = pd.concat([left_part, right_part_shifted], axis=1)\n",
    "    \n",
    "    df_negative['label'] = 0\n",
    "    \n",
    "    return df_negative\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### create partition of the data so it fits into memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_parts = 10\n",
    "part = 0\n",
    "\n",
    "train_users_features, train_users_tokens, train_reviews_features, train_reviews_tokens, train_matches = create_subsets(train_users_features, train_users_tokens, train_reviews_features, train_reviews_tokens, train_matches, total_parts=total_parts, part=part)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creates text embeddings and merge the users and reviews datasets. these are the positive samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = 'model.pt'\n",
    "train_user_embeddings, train_review_embeddings = embed_data(train_users_tokens, train_reviews_tokens, model_path, batch_size=64, device=device)\n",
    "\n",
    "train_users_features.reset_index(drop=True, inplace=True)\n",
    "train_reviews_features.reset_index(drop=True, inplace=True)\n",
    "train_user_embeddings.reset_index(drop=True, inplace=True)\n",
    "train_review_embeddings.reset_index(drop=True, inplace=True)\n",
    "\n",
    "train_users = pd.concat([train_users_features, train_user_embeddings], axis=1)\n",
    "train_reviews = pd.concat([train_reviews_features, train_review_embeddings], axis=1)\n",
    "\n",
    "merged_users = train_matches.merge(train_users, on='user_id', how='left')\n",
    "final_train = merged_users.merge(train_reviews, on='review_id', how='left')\n",
    "\n",
    "final_train = final_train.drop(columns=['accommodation_id_y', 'accommodation_id'])\n",
    "final_train = final_train.rename(columns={'accommodation_id_x': 'accommodation_id'})\n",
    "final_train['label'] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creates negative samples by shifting the data - this allows us to use reviews for same accommodations which will learn meaningful differences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_samples = generate_negative_samples_shift(final_train, 488)\n",
    "negative_samples['label'] = 0\n",
    "\n",
    "train = pd.concat([final_train, negative_samples], ignore_index=True)\n",
    "train = train.sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trains the model on the subset of data, we used it several times and inferences using an ensemble of 5 models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train.drop(columns=['user_id', 'accommodation_id', 'review_id', 'label'])\n",
    "y = train['label']\n",
    "train_data = lgb.Dataset(X, label=y)\n",
    "\n",
    "params = {\n",
    "    'objective': 'binary',\n",
    "    'metric': 'binary_error',\n",
    "    'learning_rate': 0.1,\n",
    "    'num_leaves': 31,\n",
    "    'feature_fraction': 0.8,\n",
    "    'bagging_fraction': 0.8,\n",
    "    'bagging_freq': 5,\n",
    "    'n_estimators': 100\n",
    "}\n",
    "\n",
    "model = lgb.train(params, train_data, num_boost_round=1000)\n",
    "model.save_model(f'model_{part}.txt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
