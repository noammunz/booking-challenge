{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import copy\n",
    "import pickle\n",
    "from tqdm.notebook import tqdm\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, Sampler\n",
    "from transformers import AutoModel\n",
    "from transformers import get_cosine_schedule_with_warmup\n",
    "import os\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import builtins\n",
    "from datetime import datetime\n",
    "get_time = lambda: f\"[{datetime.now():%H:%M:%S}]\"\n",
    "original_print = builtins.print\n",
    "builtins.print = lambda *args, **kwargs: original_print(get_time(), \"   \", *args, **kwargs)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training script for the embedder models to be used before the boosting model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed=42, torch_stuff=True):\n",
    "    \"\"\"\n",
    "    Seeds everything for reproducibility.\n",
    "    \"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    if torch_stuff:\n",
    "        torch.manual_seed(seed)\n",
    "        torch.use_deterministic_algorithms(True)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    os.environ['CUBLAS_WORKSPACE_CONFIG'] = ':4096:8'\n",
    "\n",
    "    if torch_stuff and torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_users = pd.read_parquet('val_users_tokens.parquet')\n",
    "val_reviews = pd.read_parquet('val_reviews_tokens.parquet')\n",
    "val_matches = pd.read_csv('val_matches.csv')\n",
    "\n",
    "val_users_indexed = val_users.set_index('user_id')\n",
    "val_reviews_indexed = val_reviews.set_index(['review_id', 'accommodation_id'])\n",
    "\n",
    "train_reviews = pd.read_parquet('train_reviews_tokens.parquet')\n",
    "train_matches = pd.read_csv('train_matches.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function and classes used in training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_sequence(sequences, batch_first=False, padding_value=0):\n",
    "    \"\"\"\n",
    "    Pad a list of variable length sequences with padding_value.\n",
    "    \"\"\"\n",
    "    max_len = max(len(seq) for seq in sequences)\n",
    "    padded_sequences = []\n",
    "\n",
    "    for seq in sequences:\n",
    "        padding = torch.full((max_len - len(seq),), padding_value, dtype=seq.dtype)\n",
    "        padded_sequences.append(torch.cat((seq, padding)))\n",
    "\n",
    "    if batch_first:\n",
    "        return torch.stack(padded_sequences)\n",
    "    else:\n",
    "        return torch.stack(padded_sequences).transpose(0, 1)\n",
    "    \n",
    "def collate_fn(batch):\n",
    "    \"\"\"\n",
    "    Collate function for DataLoader.\n",
    "    \"\"\"\n",
    "    user_input_ids = [item['user_input_ids'] for item in batch]\n",
    "    user_attention_mask = [item['user_attention_mask'] for item in batch]\n",
    "    review_input_ids = [item['review_input_ids'] for item in batch]\n",
    "    review_attention_mask = [item['review_attention_mask'] for item in batch]\n",
    "    \n",
    "    user_input_ids = pad_sequence(user_input_ids, batch_first=True, padding_value=0)\n",
    "    user_attention_mask = pad_sequence(user_attention_mask, batch_first=True, padding_value=0)\n",
    "    review_input_ids = pad_sequence(review_input_ids, batch_first=True, padding_value=0)\n",
    "    review_attention_mask = pad_sequence(review_attention_mask, batch_first=True, padding_value=0)\n",
    "    \n",
    "    return {\n",
    "        'user_input_ids': user_input_ids,\n",
    "        'user_attention_mask': user_attention_mask,\n",
    "        'review_input_ids': review_input_ids,\n",
    "        'review_attention_mask': review_attention_mask,\n",
    "    }\n",
    "\n",
    "class TrainDataset(Dataset):\n",
    "    def __init__(self, users_dict, reviews_dict, positive_pairs, group_to_indices):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            users_dict (dict): Dictionary with user_id as key and tuple of input_ids and attention_mask as value.\n",
    "            reviews_dict (dict): Dictionary with review_id as key and tuple of input_ids and attention_mask as value.\n",
    "            positive_pairs (list): List of tuples (user_id, review_id).\n",
    "            group_to_indices (dict): Dictionary with group_id as key and list of indices as value.\n",
    "        \"\"\"\n",
    "        self.users_dict = users_dict\n",
    "        self.reviews_dict = reviews_dict\n",
    "        self.positive_pairs = positive_pairs\n",
    "        self.group_to_indices = group_to_indices\n",
    "        self.groups = list(group_to_indices.keys())\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.positive_pairs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        user_id, review_id = self.positive_pairs[idx]\n",
    "        user_input_ids, user_attention_mask = self.users_dict[user_id]\n",
    "        review_input_ids, review_attention_mask = self.reviews_dict[review_id]\n",
    "        \n",
    "        # to tensors\n",
    "        user_input_ids = torch.tensor(user_input_ids)\n",
    "        user_attention_mask = torch.tensor(user_attention_mask)\n",
    "        review_input_ids = torch.tensor(review_input_ids)\n",
    "        review_attention_mask = torch.tensor(review_attention_mask)\n",
    "\n",
    "        return {\n",
    "            'user_input_ids': user_input_ids,\n",
    "            'user_attention_mask': user_attention_mask,\n",
    "            'review_input_ids': review_input_ids,\n",
    "            'review_attention_mask': review_attention_mask,\n",
    "        }\n",
    "\n",
    "class GroupBatchSampler(Sampler):\n",
    "    def __init__(self, group_to_indices, batch_size, drop_last=False):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            group_to_indices (dict): Dictionary with group_id as key and list of indices as value.\n",
    "            batch_size (int): Size of mini-batch.\n",
    "            drop_last (bool): If True, the sampler will drop the last batch if its size would be less than batch_size.\n",
    "        \"\"\"\n",
    "        self.group_to_indices = group_to_indices\n",
    "        self.batch_size = batch_size\n",
    "        self.drop_last = drop_last\n",
    "        self.groups = list(group_to_indices.keys())\n",
    "        \n",
    "    def __iter__(self):\n",
    "        all_batches = []\n",
    "        \n",
    "        for group_id in self.groups:\n",
    "            indices = self.group_to_indices[group_id]\n",
    "            \n",
    "            if len(indices) <= self.batch_size:\n",
    "                all_batches.append(indices)\n",
    "            else:\n",
    "                indices = np.array(indices)\n",
    "                np.random.shuffle(indices)\n",
    "                \n",
    "                for i in range(0, len(indices) - self.batch_size + 1, self.batch_size):\n",
    "                    batch = indices[i:i + self.batch_size].tolist()\n",
    "                    all_batches.append(batch)\n",
    "                \n",
    "                leftover = len(indices) % self.batch_size\n",
    "                if leftover > 0 and not self.drop_last:\n",
    "                    last_batch = indices[-leftover:].tolist()\n",
    "                    all_batches.append(last_batch)\n",
    "        \n",
    "        np.random.shuffle(all_batches)\n",
    "        return iter(all_batches)\n",
    "    \n",
    "    def __len__(self):\n",
    "        total_batches = 0\n",
    "        for indices in self.group_to_indices.values():\n",
    "            if len(indices) <= self.batch_size:\n",
    "                total_batches += 1\n",
    "            else:\n",
    "                n_full_batches = len(indices) // self.batch_size\n",
    "                total_batches += n_full_batches\n",
    "                if not self.drop_last and len(indices) % self.batch_size > 0:\n",
    "                    total_batches += 1\n",
    "        return total_batches\n",
    "\n",
    "class ContrastiveLoss(nn.Module):\n",
    "    def __init__(self, margin=0.5, temperature=0.07):\n",
    "        super(ContrastiveLoss, self).__init__()\n",
    "        self.margin = margin\n",
    "        self.temperature = temperature\n",
    "\n",
    "    def forward(self, user_embeddings, review_embeddings):\n",
    "        user_embeddings = F.normalize(user_embeddings, p=2, dim=1)\n",
    "        review_embeddings = F.normalize(review_embeddings, p=2, dim=1)\n",
    "\n",
    "        similarity = torch.mm(user_embeddings, review_embeddings.t()) / self.temperature\n",
    "        similarity = torch.sigmoid(similarity)\n",
    "        \n",
    "        labels = torch.arange(similarity.size(0)).to(similarity.device)\n",
    "        \n",
    "        loss = F.cross_entropy(similarity, labels)\n",
    "        \n",
    "        return loss\n",
    "\n",
    "class TwoTowersNetwork(nn.Module):\n",
    "    def __init__(self, model_id):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            model_id (str): Model identifier.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.bert1 = AutoModel.from_pretrained(model_id)\n",
    "        self.bert2 = AutoModel.from_pretrained(model_id)\n",
    "        \n",
    "    def mean_pooling(self, model_output, attention_mask):\n",
    "        token_embeddings = model_output[0]\n",
    "        mask = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "        return torch.sum(token_embeddings * mask, 1) / torch.clamp(mask.sum(1), min=1e-9)\n",
    "\n",
    "    def forward(self, context_ids, context_mask, review_ids, review_mask):\n",
    "        context_output = self.bert1(context_ids, attention_mask=context_mask)\n",
    "        review_output = self.bert2(review_ids, attention_mask=review_mask)\n",
    "        \n",
    "        context_embed = self.mean_pooling(context_output, context_mask)\n",
    "        review_embed = self.mean_pooling(review_output, review_mask)\n",
    "        \n",
    "        return context_embed, review_embed\n",
    "\n",
    "def get_scheduler(optimizer, num_training_steps, warmup_ratio=0.02):\n",
    "    \"\"\"\n",
    "    Get a scheduler with cosine annealing\n",
    "    Args:\n",
    "        optimizer (torch.optim.Optimizer): Optimizer.\n",
    "        num_training_steps (int): Number of training steps.\n",
    "        warmup_ratio (float): Warmup ratio.\n",
    "    Returns:\n",
    "        scheduler: Scheduler.\n",
    "    \"\"\"\n",
    "    num_warmup_steps = int(num_training_steps * warmup_ratio)\n",
    "    \n",
    "    scheduler = get_cosine_schedule_with_warmup(\n",
    "        optimizer,\n",
    "        num_warmup_steps=num_warmup_steps,\n",
    "        num_training_steps=num_training_steps,\n",
    "        num_cycles=2  \n",
    "    )\n",
    "    \n",
    "    return scheduler\n",
    "\n",
    "def get_val_item(val_users_indexed, val_reviews_indexed, val_matches, idx):\n",
    "    \"\"\"\n",
    "    Get a validation item.\n",
    "    Args:\n",
    "        val_users_indexed (pd.DataFrame): Indexed validation users.\n",
    "        val_reviews_indexed (pd.DataFrame): Indexed validation reviews.\n",
    "        val_matches (pd.DataFrame): Validation matches.\n",
    "        idx (int): Index of the item.\n",
    "    Returns:\n",
    "        dict: Dictionary with user_id, accommodation_id, review_id, user_tokens, review_tokens and review_ids.\n",
    "    \"\"\"\n",
    "    user_id = val_matches.loc[idx, 'user_id']\n",
    "    accommodation_id = val_matches.loc[idx, 'accommodation_id']\n",
    "    review_id = val_matches.loc[idx, 'review_id']\n",
    "    \n",
    "    user_tokens = (\n",
    "        np.array(val_users_indexed.loc[user_id].iloc[-2]), \n",
    "        np.array(val_users_indexed.loc[user_id].iloc[-1])\n",
    "    )\n",
    "\n",
    "    def pad_sequence(seq, max_len=512, pad_value=1):\n",
    "        return np.pad(seq, (0, max_len - len(seq)), mode='constant', constant_values=pad_value)\n",
    "    \n",
    "    input_ids_list = val_reviews_indexed.loc[(slice(None), accommodation_id), :].values[:, -2]\n",
    "    attention_mask_list = val_reviews_indexed.loc[(slice(None), accommodation_id), :].values[:, -1]\n",
    "    padded_input_ids = np.stack([pad_sequence(x, max_len=512, pad_value=1) for x in input_ids_list])\n",
    "    padded_attention_mask = np.stack([pad_sequence(x, max_len=512, pad_value=0) for x in attention_mask_list])\n",
    "\n",
    "    review_tokens = (padded_input_ids, padded_attention_mask)\n",
    "    review_ids = val_reviews_indexed.loc[(slice(None), accommodation_id), :].index.get_level_values('review_id').tolist()\n",
    "\n",
    "    return {\n",
    "        'user_id': user_id,\n",
    "        'review_id': review_id,\n",
    "        'user_tokens': user_tokens,\n",
    "        'review_tokens': review_tokens,\n",
    "        'review_ids': review_ids\n",
    "    }\n",
    "\n",
    "def evaluate(model, val_users_indexed, val_reviews_indexed, val_matches, device, criterion, batch_size=64, num_samples=3000):\n",
    "    \"\"\"\n",
    "    Evaluate the model.\n",
    "    Args:\n",
    "        model: Model.\n",
    "        val_users_indexed (pd.DataFrame): Indexed validation users.\n",
    "        val_reviews_indexed (pd.DataFrame): Indexed validation reviews.\n",
    "        val_matches (pd.DataFrame): Validation matches.\n",
    "        device: Device.\n",
    "        criterion: Criterion.\n",
    "        batch_size (int): Batch size.\n",
    "        num_samples (int): Number of samples to evaluate.\n",
    "    Returns:\n",
    "        dict: Dictionary with MRR, Hit@10 and loss.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    mrr_scores = []\n",
    "    hit_rates = []\n",
    "    total_loss = 0\n",
    "    indices = np.random.choice(len(val_matches), num_samples, replace=False)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for ii, idx in enumerate(indices):\n",
    "            batch = get_val_item(val_users_indexed, val_reviews_indexed, val_matches, idx)\n",
    "            \n",
    "            user_input_ids = torch.tensor(batch['user_tokens'][0], dtype=torch.long).unsqueeze(0).to(device)\n",
    "            user_attention_mask = torch.tensor(batch['user_tokens'][1], dtype=torch.long).unsqueeze(0).to(device)\n",
    "            \n",
    "            review_input_ids = torch.tensor(batch['review_tokens'][0], dtype=torch.long)\n",
    "            review_attention_mask = torch.tensor(batch['review_tokens'][1], dtype=torch.long)\n",
    "            \n",
    "            true_review_id = batch['review_id']\n",
    "            review_ids = batch['review_ids']\n",
    "\n",
    "            all_scores = []\n",
    "            batch_loss = 0\n",
    "            \n",
    "            for j in range(0, len(review_input_ids), batch_size):\n",
    "                batch_review_input_ids = review_input_ids[j:j+batch_size].to(device)\n",
    "                batch_review_attention_mask = review_attention_mask[j:j+batch_size].to(device)\n",
    "                \n",
    "                batch_user_input_ids = user_input_ids.repeat(len(batch_review_input_ids), 1)\n",
    "                batch_user_attention_mask = user_attention_mask.repeat(len(batch_review_input_ids), 1)\n",
    "                \n",
    "                user_emb, review_emb = model(\n",
    "                    batch_user_input_ids,\n",
    "                    batch_user_attention_mask,\n",
    "                    batch_review_input_ids,\n",
    "                    batch_review_attention_mask\n",
    "                )\n",
    "                \n",
    "                scores = torch.sum(user_emb * review_emb, dim=1)\n",
    "                all_scores.append(scores)\n",
    "                \n",
    "                batch_loss += criterion(user_emb, review_emb)\n",
    "\n",
    "            all_scores = torch.cat(all_scores).cpu().numpy()\n",
    "            \n",
    "            pairs = list(zip(all_scores, review_ids))\n",
    "            sorted_pairs = sorted(pairs, key=lambda x: x[0], reverse=True)\n",
    "            sorted_review_ids = [rid for _, rid in sorted_pairs][:10]\n",
    "            \n",
    "            try:\n",
    "                rank = sorted_review_ids.index(true_review_id) + 1\n",
    "                mrr = 1.0 / rank\n",
    "                hit_rates.append(1 if rank <= 10 else 0)\n",
    "            except ValueError:\n",
    "                mrr = 0.0\n",
    "                hit_rates.append(0)\n",
    "            \n",
    "            mrr_scores.append(mrr)\n",
    "            total_loss += batch_loss.item()\n",
    "\n",
    "            if (ii + 1) % 250 == 0:\n",
    "                print(f\"Processed {idx+1}/{num_samples} samples, \"\n",
    "                      f\"MRR: {np.mean(mrr_scores):.4f}, \"\n",
    "                      f\"Hit@10: {np.mean(hit_rates):.4f}, \"\n",
    "                      f\"Loss: {total_loss/(idx+1):.4f}\", flush=True)\n",
    "\n",
    "    metrics = {\n",
    "        'mrr': np.mean(mrr_scores),\n",
    "        'hit_rate': np.mean(hit_rates),\n",
    "        'loss': total_loss/num_samples\n",
    "    }\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "def train_model(kmeans_loader, accomodation_loader, val_users_indexed, val_reviews_indexed, val_matches, \n",
    "                model, num_epochs, learning_rate, device, max_grad_norm=1.0, batch_size=32):    \n",
    "    \"\"\"\n",
    "    Train a model.\n",
    "    Args:\n",
    "        kmeans_loader (DataLoader): DataLoader based on kmeans groups.\n",
    "        accomodation_loader (DataLoader): DataLoader based on accomodation groups.\n",
    "        val_users_indexed (pd.DataFrame): Indexed validation users.\n",
    "        val_reviews_indexed (pd.DataFrame): Indexed validation reviews.\n",
    "        val_matches (pd.DataFrame): Validation matches.\n",
    "        model: Model.\n",
    "        num_epochs (int): Number of epochs.\n",
    "        learning_rate (float): Learning rate.\n",
    "        device: Device.\n",
    "        max_grad_norm (float): Maximum gradient norm.\n",
    "        batch_size (int): Batch size.\n",
    "    Returns:\n",
    "        dict: Training history.\n",
    "    \"\"\"\n",
    "    model = model.to(device)\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "    criterion = ContrastiveLoss()\n",
    "    total_samples = len(kmeans_loader.dataset) + len(accomodation_loader.dataset)\n",
    "\n",
    "    num_training_steps = num_epochs * total_samples // batch_size\n",
    "    scheduler = get_scheduler(optimizer, num_training_steps)\n",
    "    \n",
    "    history = {\n",
    "        'train_loss': [],\n",
    "        'val_mrr': [],\n",
    "        'val_hit_rate': [],\n",
    "        'best_mrr': 0,\n",
    "        'best_epoch': 0,\n",
    "        'best_model': None,\n",
    "        'learning_rates': []\n",
    "    }\n",
    "    \n",
    "    best_mrr = 0\n",
    "    \n",
    "    kmeans_iter = iter(kmeans_loader)\n",
    "    accomodation_iter = iter(accomodation_loader)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        epoch_loss = 0\n",
    "        num_batches = total_samples // batch_size\n",
    "        \n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "        history['learning_rates'].append(current_lr)\n",
    "        \n",
    "        for batch_idx, in range(num_batches):\n",
    "            if batch_idx % 2 == 0:  \n",
    "                try:\n",
    "                    kmeans_batch = next(kmeans_iter)\n",
    "                    user_input_ids = kmeans_batch['user_input_ids'].to(device)\n",
    "                    user_attention_mask = kmeans_batch['user_attention_mask'].to(device)\n",
    "                    review_input_ids = kmeans_batch['review_input_ids'].to(device)\n",
    "                    review_attention_mask = kmeans_batch['review_attention_mask'].to(device)\n",
    "                except StopIteration:\n",
    "                    kmeans_iter = iter(kmeans_loader)\n",
    "                    kmeans_batch = next(kmeans_iter)\n",
    "                    user_input_ids = kmeans_batch['user_input_ids'].to(device)\n",
    "                    user_attention_mask = kmeans_batch['user_attention_mask'].to(device)\n",
    "                    review_input_ids = kmeans_batch['review_input_ids'].to(device)\n",
    "                    review_attention_mask = kmeans_batch['review_attention_mask'].to(device)\n",
    "            else:  \n",
    "                try:\n",
    "                    accomodation_batch = next(accomodation_iter)\n",
    "                    user_input_ids = accomodation_batch['user_input_ids'].to(device)\n",
    "                    user_attention_mask = accomodation_batch['user_attention_mask'].to(device)\n",
    "                    review_input_ids = accomodation_batch['review_input_ids'].to(device)\n",
    "                    review_attention_mask = accomodation_batch['review_attention_mask'].to(device)\n",
    "                except StopIteration:\n",
    "                    accomodation_iter = iter(accomodation_loader)\n",
    "                    accomodation_batch = next(accomodation_iter)\n",
    "                    user_input_ids = accomodation_batch['user_input_ids'].to(device)\n",
    "                    user_attention_mask = accomodation_batch['user_attention_mask'].to(device)\n",
    "                    review_input_ids = accomodation_batch['review_input_ids'].to(device)\n",
    "                    review_attention_mask = accomodation_batch['review_attention_mask'].to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            user_embeddings, review_embeddings = model(\n",
    "                user_input_ids,\n",
    "                user_attention_mask,\n",
    "                review_input_ids,\n",
    "                review_attention_mask\n",
    "            )\n",
    "            \n",
    "            loss = criterion(user_embeddings, review_embeddings)\n",
    "            \n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            epoch_loss += loss.item()\n",
    "            \n",
    "            if (batch_idx + 1) % 1500 == 0:\n",
    "                current_lr = optimizer.param_groups[0]['lr']\n",
    "                print(f'Epoch [{epoch+1}/{num_epochs}], '\n",
    "                      f'Step [{(batch_idx+1)}/{num_batches}], '\n",
    "                      f'LR: {current_lr:.6f}, '\n",
    "                      f'Running Loss: {epoch_loss/(batch_idx+1):.4f}', flush=True)\n",
    "\n",
    "        avg_epoch_loss = epoch_loss / num_batches\n",
    "        history['train_loss'].append(avg_epoch_loss)\n",
    "        \n",
    "        print(f\"\\nValidating epoch {epoch+1}...\", flush=True)\n",
    "        val_metrics = evaluate(\n",
    "            model=model,\n",
    "            val_users_indexed=val_users_indexed,\n",
    "            val_reviews_indexed=val_reviews_indexed,\n",
    "            val_matches=val_matches,\n",
    "            device=device,\n",
    "            criterion=criterion,\n",
    "            batch_size=batch_size\n",
    "        )\n",
    "        \n",
    "        history['val_mrr'].append(val_metrics['mrr'])\n",
    "        history['val_hit_rate'].append(val_metrics['hit_rate'])\n",
    "        \n",
    "        print(f\"Epoch {epoch+1} Summary:\", flush=True)\n",
    "        print(f\"Train Loss: {avg_epoch_loss:.4f}\", flush=True)\n",
    "        print(f\"Val MRR: {val_metrics['mrr']:.4f}\", flush=True)\n",
    "        print(f\"Val Hit@10: {val_metrics['hit_rate']:.4f}\", flush=True)\n",
    "        print(f\"Val Loss: {val_metrics['loss']:.4f}\\n\", flush=True)\n",
    "        \n",
    "        if val_metrics['mrr'] > best_mrr:\n",
    "            best_mrr = val_metrics['mrr']\n",
    "            history['best_mrr'] = best_mrr\n",
    "            history['best_epoch'] = epoch\n",
    "            history['best_model'] = copy.deepcopy(model.state_dict())\n",
    "            torch.save(model.state_dict(), f'model_{best_mrr:.4f}.pt')\n",
    "            print(f\"New best MRR: {best_mrr:.4f}\\n\", flush=True)\n",
    "        \n",
    "        print(\"-\" * 60 + \"\\n\", flush=True)\n",
    "    \n",
    "    print(f\"Training completed. Best MRR: {best_mrr:.4f} at epoch {history['best_epoch']+1}\", flush=True)\n",
    "    \n",
    "    return history\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load objects and create dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('users_dict.pkl', 'rb') as f:\n",
    "    users_dict = pickle.load(f)\n",
    "\n",
    "with open('reviews_dict.pkl', 'rb') as f:\n",
    "    reviews_dict = pickle.load(f)\n",
    "\n",
    "with open('positive_pairs.pkl', 'rb') as f:\n",
    "    positive_pairs = pickle.load(f)\n",
    "\n",
    "with open('kmeans_groups.pkl', 'rb') as f:\n",
    "    kmeans_groups = pickle.load(f)\n",
    "\n",
    "kmeans_train_dataset = TrainDataset(users_dict, reviews_dict, positive_pairs, kmeans_groups)\n",
    "kmeans_sampler = GroupBatchSampler(kmeans_groups, batch_size=32, drop_last=True)\n",
    "kmeans_loader = DataLoader(kmeans_train_dataset, batch_sampler=kmeans_sampler, collate_fn=collate_fn)\n",
    "\n",
    "accomodation_groups = train_matches.groupby('accommodation_id').apply(lambda x: x.index.tolist()).to_dict()\n",
    "accomodation_train_dataset = TrainDataset(users_dict, reviews_dict, positive_pairs, accomodation_groups)\n",
    "accomodation_sampler = GroupBatchSampler(accomodation_groups, batch_size=32, drop_last=True)\n",
    "accomodation_loader = DataLoader(accomodation_train_dataset, batch_sampler=accomodation_sampler, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set training parameters and train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 3\n",
    "learning_rate = 2e-5\n",
    "batch_size = 32\n",
    "\n",
    "model_id = \"sentence-transformers/all-MiniLM-L12-v2\"\n",
    "\n",
    "seed_everything(42)\n",
    "model = TwoTowersNetwork(model_id)\n",
    "history = train_model(\n",
    "    train_loader=kmeans_loader,\n",
    "    val_users_indexed=val_users_indexed,\n",
    "    val_reviews_indexed=val_reviews_indexed,\n",
    "    val_matches=val_matches,\n",
    "    model=model,\n",
    "    num_epochs=epochs,\n",
    "    learning_rate=learning_rate,\n",
    "    device=device,\n",
    "    batch_size=batch_size\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
