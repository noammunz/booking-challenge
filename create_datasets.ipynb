{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, Sampler\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "pd.set_option('display.max_columns', 5000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The purpose of this notebook is to create the necessary objects for dataset creation during training, aiming to save time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_users = pd.read_parquet('train_users_tokens.parquet')\n",
    "train_reviews = pd.read_parquet('train_reviews_tokens.parquet')\n",
    "train_matches = pd.read_csv('train_matches.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "users_dict = {}\n",
    "for i, row in tqdm(train_users.iterrows(), total=len(train_users), desc=\"Creating users dict\"):\n",
    "    user_id = row['user_id']\n",
    "    input_ids = row['input_ids']\n",
    "    attention_mask = row['attention_mask']\n",
    "    users_dict[user_id] = (input_ids, attention_mask)\n",
    "\n",
    "reviews_dict = {}\n",
    "for i, row in tqdm(train_reviews.iterrows(), total=len(train_reviews), desc=\"Creating reviews dict\"):\n",
    "    review_id = row['review_id']\n",
    "    input_ids = row['input_ids']\n",
    "    attention_mask = row['attention_mask']\n",
    "    reviews_dict[review_id] = (input_ids, attention_mask)\n",
    "\n",
    "positive_pairs = []\n",
    "for i, row in tqdm(train_matches.iterrows(), total=len(train_matches), desc=\"Creating positive pairs\"):\n",
    "    user_id = row['user_id']\n",
    "    review_id = row['review_id']\n",
    "    accommodation_id = row['accommodation_id']\n",
    "    positive_pairs.append((user_id, review_id))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('users_dict.pkl', 'wb') as f:\n",
    "    pickle.dump(users_dict, f)\n",
    "\n",
    "with open('reviews_dict.pkl', 'wb') as f:\n",
    "    pickle.dump(reviews_dict, f)\n",
    "\n",
    "with open('positive_pairs.pkl', 'wb') as f:\n",
    "    pickle.dump(positive_pairs, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example of loading these objects and creating datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_sequence(sequences, batch_first=False, padding_value=0):\n",
    "    \"\"\"\n",
    "    Pad a list of variable length sequences with padding_value.\n",
    "    \"\"\"\n",
    "    max_len = max(len(seq) for seq in sequences)\n",
    "    padded_sequences = []\n",
    "\n",
    "    for seq in sequences:\n",
    "        padding = torch.full((max_len - len(seq),), padding_value, dtype=seq.dtype)\n",
    "        padded_sequences.append(torch.cat((seq, padding)))\n",
    "\n",
    "    if batch_first:\n",
    "        return torch.stack(padded_sequences)\n",
    "    else:\n",
    "        return torch.stack(padded_sequences).transpose(0, 1)\n",
    "    \n",
    "def collate_fn(batch):\n",
    "    \"\"\"\n",
    "    Collate function for DataLoader.\n",
    "    \"\"\"\n",
    "    user_input_ids = [item['user_input_ids'] for item in batch]\n",
    "    user_attention_mask = [item['user_attention_mask'] for item in batch]\n",
    "    review_input_ids = [item['review_input_ids'] for item in batch]\n",
    "    review_attention_mask = [item['review_attention_mask'] for item in batch]\n",
    "    \n",
    "    user_input_ids = pad_sequence(user_input_ids, batch_first=True, padding_value=0)\n",
    "    user_attention_mask = pad_sequence(user_attention_mask, batch_first=True, padding_value=0)\n",
    "    review_input_ids = pad_sequence(review_input_ids, batch_first=True, padding_value=0)\n",
    "    review_attention_mask = pad_sequence(review_attention_mask, batch_first=True, padding_value=0)\n",
    "    \n",
    "    return {\n",
    "        'user_input_ids': user_input_ids,\n",
    "        'user_attention_mask': user_attention_mask,\n",
    "        'review_input_ids': review_input_ids,\n",
    "        'review_attention_mask': review_attention_mask,\n",
    "    }\n",
    "\n",
    "class TrainDataset(Dataset):\n",
    "    def __init__(self, users_dict, reviews_dict, positive_pairs, group_to_indices):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            users_dict (dict): Dictionary with user_id as key and tuple of input_ids and attention_mask as value.\n",
    "            reviews_dict (dict): Dictionary with review_id as key and tuple of input_ids and attention_mask as value.\n",
    "            positive_pairs (list): List of tuples (user_id, review_id).\n",
    "            group_to_indices (dict): Dictionary with group_id as key and list of indices as value.\n",
    "        \"\"\"\n",
    "        self.users_dict = users_dict\n",
    "        self.reviews_dict = reviews_dict\n",
    "        self.positive_pairs = positive_pairs\n",
    "        self.group_to_indices = group_to_indices\n",
    "        self.groups = list(group_to_indices.keys())\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.positive_pairs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        user_id, review_id = self.positive_pairs[idx]\n",
    "        user_input_ids, user_attention_mask = self.users_dict[user_id]\n",
    "        review_input_ids, review_attention_mask = self.reviews_dict[review_id]\n",
    "        \n",
    "        # to tensors\n",
    "        user_input_ids = torch.tensor(user_input_ids)\n",
    "        user_attention_mask = torch.tensor(user_attention_mask)\n",
    "        review_input_ids = torch.tensor(review_input_ids)\n",
    "        review_attention_mask = torch.tensor(review_attention_mask)\n",
    "\n",
    "        return {\n",
    "            'user_input_ids': user_input_ids,\n",
    "            'user_attention_mask': user_attention_mask,\n",
    "            'review_input_ids': review_input_ids,\n",
    "            'review_attention_mask': review_attention_mask,\n",
    "        }\n",
    "\n",
    "class GroupBatchSampler(Sampler):\n",
    "    def __init__(self, group_to_indices, batch_size, drop_last=False):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            group_to_indices (dict): Dictionary with group_id as key and list of indices as value.\n",
    "            batch_size (int): Size of mini-batch.\n",
    "            drop_last (bool): If True, the sampler will drop the last batch if its size would be less than batch_size.\n",
    "        \"\"\"\n",
    "        self.group_to_indices = group_to_indices\n",
    "        self.batch_size = batch_size\n",
    "        self.drop_last = drop_last\n",
    "        self.groups = list(group_to_indices.keys())\n",
    "        \n",
    "    def __iter__(self):\n",
    "        all_batches = []\n",
    "        \n",
    "        for group_id in self.groups:\n",
    "            indices = self.group_to_indices[group_id]\n",
    "            \n",
    "            if len(indices) <= self.batch_size:\n",
    "                all_batches.append(indices)\n",
    "            else:\n",
    "                indices = np.array(indices)\n",
    "                np.random.shuffle(indices)\n",
    "                \n",
    "                for i in range(0, len(indices) - self.batch_size + 1, self.batch_size):\n",
    "                    batch = indices[i:i + self.batch_size].tolist()\n",
    "                    all_batches.append(batch)\n",
    "                \n",
    "                leftover = len(indices) % self.batch_size\n",
    "                if leftover > 0 and not self.drop_last:\n",
    "                    last_batch = indices[-leftover:].tolist()\n",
    "                    all_batches.append(last_batch)\n",
    "        \n",
    "        np.random.shuffle(all_batches)\n",
    "        return iter(all_batches)\n",
    "    \n",
    "    def __len__(self):\n",
    "        total_batches = 0\n",
    "        for indices in self.group_to_indices.values():\n",
    "            if len(indices) <= self.batch_size:\n",
    "                total_batches += 1\n",
    "            else:\n",
    "                n_full_batches = len(indices) // self.batch_size\n",
    "                total_batches += n_full_batches\n",
    "                if not self.drop_last and len(indices) % self.batch_size > 0:\n",
    "                    total_batches += 1\n",
    "        return total_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('users_dict.pkl', 'rb') as f:\n",
    "    users_dict = pickle.load(f)\n",
    "\n",
    "with open('reviews_dict.pkl', 'rb') as f:\n",
    "    reviews_dict = pickle.load(f)\n",
    "\n",
    "with open('positive_pairs.pkl', 'rb') as f:\n",
    "    positive_pairs = pickle.load(f)\n",
    "\n",
    "with open('kmeans_groups.pkl', 'rb') as f:\n",
    "    kmeans_groups = pickle.load(f)\n",
    "\n",
    "kmeans_train_dataset = TrainDataset(users_dict, reviews_dict, positive_pairs, kmeans_groups)\n",
    "kmeans_sampler = GroupBatchSampler(kmeans_groups, batch_size=32, drop_last=True)\n",
    "kmeans_loader = DataLoader(kmeans_train_dataset, batch_sampler=kmeans_sampler, collate_fn=collate_fn)\n",
    "\n",
    "accomodation_groups = train_matches.groupby('accommodation_id').apply(lambda x: x.index.tolist()).to_dict()\n",
    "accomodation_train_dataset = TrainDataset(users_dict, reviews_dict, positive_pairs, accomodation_groups)\n",
    "accomodation_sampler = GroupBatchSampler(accomodation_groups, batch_size=32, drop_last=True)\n",
    "accomodation_loader = DataLoader(accomodation_train_dataset, batch_sampler=accomodation_sampler, collate_fn=collate_fn)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
