{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from tqdm.notebook import tqdm\n",
    "import random\n",
    "\n",
    "from textblob import TextBlob\n",
    "tqdm.pandas()\n",
    "\n",
    "import torch\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.decomposition import PCA\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import os\n",
    "\n",
    "pd.set_option('display.max_columns', 5060)\n",
    "\n",
    "def seed_everything(seed=42, torch_stuff=True):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    if torch_stuff:\n",
    "        torch.manual_seed(seed)\n",
    "        torch.use_deterministic_algorithms(True)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark  = False\n",
    "\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    os.environ['CUBLAS_WORKSPACE_CONFIG'] = ':4096:8'\n",
    "\n",
    "    if torch_stuff and torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "seed_everything()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_users = pd.read_csv('train_users.csv')\n",
    "train_reviews = pd.read_csv('train_reviews.csv')\n",
    "train_matches = pd.read_csv('train_matches.csv')\n",
    "\n",
    "val_users = pd.read_csv('val_users.csv')\n",
    "val_reviews = pd.read_csv('val_reviews.csv')\n",
    "val_matches = pd.read_csv('val_matches.csv')\n",
    "\n",
    "test_users = pd.read_csv('test_users.csv')\n",
    "test_reviews = pd.read_csv('test_reviews.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load model, both for embedding and tokenizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"sentence-transformers/all-MiniLM-L12-v2\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModel.from_pretrained(model_id)\n",
    "\n",
    "embedding_model = SentenceTransformer(model_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions for users processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "guest_country_dict = {\n",
    "    'Zuc': 'australia', 'Mejok': 'united kingdom', 'Nen': 'india', 'Tig': 'philippines', 'Nukeye': 'united states of america', 'Jof': 'canada',\n",
    "    'Fuxa': 'malaysia', 'Pule': 'romania', 'Wiz': 'brazil', 'Naz': 'china', 'Toq': 'japan', 'Zot': 'france',\n",
    "    'Rawabe': 'italy', 'Pes': 'russia', 'Xopag': 'korea, south', 'Guroz': 'spain', 'Heyuru': 'indonesia', 'Vohomi': 'turkey',\n",
    "    'Dawal': 'netherlands', 'Pihay': 'saudi arabia', 'Vig': 'switzerland', 'Fanir': 'poland', 'Modey': 'taiwan', 'Rixo': 'belgium',\n",
    "    'Cibi': 'sweden', 'Kiy': 'argentina', 'Gobuf': 'ireland', 'Cej': 'united arab emirates', 'Bemil': 'austria', 'Pikune': 'singapore',\n",
    "    'Qebuf': 'thailand', 'Vema': 'israel', 'Qehoj': 'germany', 'Naz': 'china', 'Gifop': 'norway', 'Diwej': 'bangladesh',\n",
    "    'Vezuz': 'iran', 'Vohomi': 'colombia', 'Zic': 'denmark', 'Pimes': 'greece', 'Xazas': 'new zealand', 'Wapef': 'portugal',\n",
    "    'Made': 'south africa', 'Cutip': 'croatia', 'Noliqo': 'morocco', 'Vey': 'egypt', 'Vezuz': 'czech republic', 'Qop': 'iceland',\n",
    "    'Gugen': 'sri lanka', 'Nolita': 'hungary', 'May': 'slovenia', 'Tuleho': 'bulgaria', 'Nol': 'cyprus', 'Jeqe': 'georgia',\n",
    "    'Buzi': 'mexico', 'Xogem': 'kazakhstan', 'Quduja': 'kosovo', 'Tuhi': 'finland', 'Nis': 'moldova', 'Kudefa': 'estonia',\n",
    "    'Tucit': 'latvia', 'Bemab': 'lithuania', 'Tomu': 'ukraine', 'Mal': 'costa rica'\n",
    "}\n",
    "\n",
    "month_dict = {\n",
    "    1: 'January', 2: 'February', 3: 'March', 4: 'April', 5: 'May', 6: 'June',\n",
    "    7: 'July', 8: 'August', 9: 'September', 10: 'October', 11: 'November', 12: 'December'\n",
    "}\n",
    "\n",
    "def process_guest_type(df):\n",
    "    \"\"\"\n",
    "    One-hot encode 'guest_type' column\n",
    "    Args:\n",
    "        df (pd.DataFrame): DataFrame containing 'guest_type' column\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with one-hot encoded 'guest_type' column\n",
    "    \"\"\"\n",
    "    df_guest_type_ohe = pd.get_dummies(df['guest_type'], prefix='guest_type').astype(int)\n",
    "    df = pd.concat([df, df_guest_type_ohe], axis=1)\n",
    "    df.drop('guest_type', axis=1, inplace=True)  # Drop original 'guest type' column\n",
    "    return df\n",
    "\n",
    "def process_guest_country(df, guest_country_dict, model, pca_components=16):\n",
    "    \"\"\"\n",
    "    Encode 'guest_country' column using SentenceTransformer model\n",
    "    Args:\n",
    "        df (pd.DataFrame): DataFrame containing 'guest_country' column\n",
    "        guest_country_dict (dict): Dictionary mapping guest country codes to country names\n",
    "        model (transformers.PreTrainedModel): Pretrained SentenceTransformer model\n",
    "        pca_components (int, optional): Number of PCA components. Defaults to 16.\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with encoded 'guest_country' column\n",
    "    \"\"\"\n",
    "    country_embedding_dict = {}\n",
    "    \n",
    "    unique_real_countries = set(guest_country_dict.values())\n",
    "    \n",
    "    for country in unique_real_countries:\n",
    "        country_embedding_dict[country] = model.encode([country])[0]\n",
    "    \n",
    "    country_embedding_dict['Unknown Country'] = model.encode(['Unknown Country'])[0]\n",
    "    \n",
    "    embeddings = []\n",
    "    real_countries = df['guest_country'].apply(lambda x: guest_country_dict.get(x, 'Unknown Country')).tolist()\n",
    "\n",
    "    for real_country in real_countries:\n",
    "        embeddings.append(country_embedding_dict.get(real_country, country_embedding_dict['Unknown Country']))\n",
    "    \n",
    "    pca = PCA(n_components=pca_components)\n",
    "    reduced_embeddings = pca.fit_transform(np.vstack(embeddings))\n",
    "    embedding_df = pd.DataFrame(reduced_embeddings, columns=[f'guest_country_embedding_{i}' for i in range(reduced_embeddings.shape[1])])\n",
    "\n",
    "    df = df.drop(columns=['guest_country'])\n",
    "    df = pd.concat([df, embedding_df], axis=1)\n",
    "\n",
    "    return df\n",
    "\n",
    "def process_room_nights(df):\n",
    "    \"\"\"\n",
    "    Binning 'room_nights' column and one-hot encoding the bins\n",
    "    Args:\n",
    "        df (pd.DataFrame): DataFrame containing 'room_nights' column\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with one-hot encoded 'room_nights' column\n",
    "    \"\"\"\n",
    "    bins = [0, 1, 2, 3, 4, 5, 6, 10, 16, 30, 100, float('inf')]\n",
    "    labels = ['1', '2', '3', '4', '5', '6', '7-10', '11-16', '17-30', '31-100', '100+']\n",
    "    df['room_nights_binned'] = pd.cut(df['room_nights'], bins=bins, labels=labels, right=False)\n",
    "    df_room_nights_ohe = pd.get_dummies(df['room_nights_binned'], prefix='room_nights')\n",
    "    df_room_nights_ohe = df_room_nights_ohe.astype(int)\n",
    "    df = pd.concat([df, df_room_nights_ohe], axis=1)\n",
    "    df.drop(['room_nights', 'room_nights_binned'], axis=1, inplace=True)\n",
    "    return df\n",
    "\n",
    "def process_seasons(df):\n",
    "    \"\"\"\n",
    "    Map 'month' to 'Season' and one-hot encode 'Season'\n",
    "    Args:\n",
    "        df (pd.DataFrame): DataFrame containing 'month' column\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with one-hot encoded 'Season' column\n",
    "    \"\"\"\n",
    "    season_dict = {\n",
    "        12: 'Winter', 1: 'Winter', 2: 'Winter',\n",
    "        3: 'Spring', 4: 'Spring', 5: 'Spring',\n",
    "        6: 'Summer', 7: 'Summer', 8: 'Summer',\n",
    "        9: 'Fall', 10: 'Fall', 11: 'Fall'\n",
    "    }\n",
    "    df['Season'] = df['month'].map(season_dict)\n",
    "    df_season_ohe = pd.get_dummies(df['Season'], prefix='Season').astype(int)\n",
    "    df = pd.concat([df, df_season_ohe], axis=1)\n",
    "    df.drop('Season', axis=1, inplace=True)\n",
    "    return df\n",
    "\n",
    "def process_holidays(df):\n",
    "    \"\"\"\n",
    "    Map 'month' to 'Holiday' and one-hot encode 'Holiday'\n",
    "    Args:\n",
    "        df (pd.DataFrame): DataFrame containing 'month' column\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with one-hot encoded 'Holiday' column\n",
    "    \"\"\"\n",
    "    holidays_dict = {\n",
    "        \"New Year\": 12,\n",
    "        \"Valentines Day\": 2,\n",
    "        \"Easter\": 4,\n",
    "        \"Halloween\": 10,\n",
    "        \"Christmas\": 12,\n",
    "        \"Chinese New Year\": 1,\n",
    "    }\n",
    "    df['Holiday'] = df['month'].map({v: k for k, v in holidays_dict.items()})\n",
    "    df['Holiday'] = df['Holiday'].fillna('No Holiday')\n",
    "    df_holiday_ohe = pd.get_dummies(df['Holiday'], prefix='Holiday')\n",
    "    df_holiday_ohe = df_holiday_ohe.astype(int)\n",
    "    df = pd.concat([df, df_holiday_ohe], axis=1)\n",
    "    df.drop('Holiday', axis=1, inplace=True)\n",
    "    return df\n",
    "\n",
    "def process_month(df):\n",
    "    \"\"\"\n",
    "    One-hot encode 'month' column\n",
    "    Args:\n",
    "        df (pd.DataFrame): DataFrame containing 'month' column\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with one-hot encoded 'month' column\n",
    "    \"\"\"\n",
    "    df_month_ohe = pd.get_dummies(df['month'], prefix='month')\n",
    "    df_month_ohe = df_month_ohe.astype(int)\n",
    "    df = pd.concat([df, df_month_ohe], axis=1)\n",
    "    df.drop('month', axis=1, inplace=True)\n",
    "    return df\n",
    "\n",
    "def process_accommodation_type(df):\n",
    "    \"\"\"\n",
    "    One-hot encode 'accommodation_type' column\n",
    "    Args:\n",
    "        df (pd.DataFrame): DataFrame containing 'accommodation_type' column\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with one-hot encoded 'accommodation_type' column\n",
    "    \"\"\"\n",
    "    df_accommodation_type_ohe = pd.get_dummies(df['accommodation_type'], prefix='accommodation_type')\n",
    "    df_accommodation_type_ohe = df_accommodation_type_ohe.astype(int)\n",
    "    df = pd.concat([df, df_accommodation_type_ohe], axis=1)\n",
    "    df.drop('accommodation_type', axis=1, inplace=True)\n",
    "    return df\n",
    "\n",
    "def process_accommodation_country(df, model, pca_components=16):\n",
    "    \"\"\"\n",
    "    Encode 'accommodation_country' column using SentenceTransformer model\n",
    "    Args:\n",
    "        df (pd.DataFrame): DataFrame containing 'accommodation_country' column\n",
    "        model (transformers.PreTrainedModel): Pretrained SentenceTransformer model\n",
    "        pca_components (int, optional): Number of PCA components. Defaults to 16.\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with encoded 'accommodation_country' column\n",
    "    \"\"\"\n",
    "    embeddings = []\n",
    "    countries = df['accommodation_country'].tolist()\n",
    "    country_embedding_dict = {}\n",
    "    country_embedding_dict['Unknown Country'] = model.encode(['Unknown Country'])[0]\n",
    "\n",
    "    for country in set(countries):\n",
    "        country_embedding_dict[country] = model.encode([country])[0]\n",
    "    \n",
    "    for country in countries:\n",
    "        embeddings.append(country_embedding_dict.get(country, country_embedding_dict['Unknown Country']))\n",
    "\n",
    "    pca = PCA(n_components=pca_components)\n",
    "    reduced_embeddings = pca.fit_transform(np.vstack(embeddings))\n",
    "    embedding_df = pd.DataFrame(reduced_embeddings, columns=[f'accommodation_country_embedding_{i}' for i in range(reduced_embeddings.shape[1])])\n",
    "\n",
    "    df = df.drop(columns=['accommodation_country'])\n",
    "    df = pd.concat([df, embedding_df], axis=1)\n",
    "\n",
    "    return df\n",
    "\n",
    "def process_accommodation_score(df, scaler=None):\n",
    "    \"\"\" \n",
    "    Normalize 'accommodation_score' column\n",
    "    Args:\n",
    "        df (pd.DataFrame): DataFrame containing 'accommodation_score' column\n",
    "        scaler (sklearn.preprocessing.StandardScaler, optional): Scaler to be used. Defaults to None.\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with normalized 'accommodation_score' column\n",
    "    \"\"\"\n",
    "    if scaler is None:\n",
    "        scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "        scaler.fit(df[['accommodation_score']])\n",
    "        df['accommodation_score'] = scaler.transform(df[['accommodation_score']])\n",
    "        return df, scaler\n",
    "    else:\n",
    "        df['accommodation_score'] = scaler.transform(df[['accommodation_score']])\n",
    "        return df\n",
    "    \n",
    "def process_accommodation_star_rating(df, scaler=None):\n",
    "    \"\"\"\n",
    "    Normalize 'accommodation_star_rating' column\n",
    "    Args:\n",
    "        df (pd.DataFrame): DataFrame containing 'accommodation_star_rating' column\n",
    "        scaler (sklearn.preprocessing.StandardScaler, optional): Scaler to be used. Defaults to None.\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with normalized 'accommodation_star_rating' column\n",
    "    \"\"\"\n",
    "    if scaler is None:\n",
    "        scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "        scaler.fit(df[['accommodation_star_rating']])\n",
    "        df['accommodation_star_rating'] = scaler.transform(df[['accommodation_star_rating']])\n",
    "        return df, scaler\n",
    "    else:\n",
    "        df['accommodation_star_rating'] = scaler.transform(df[['accommodation_star_rating']])\n",
    "        return df\n",
    "\n",
    "def get_row_text(row, guest_country_dict, month_dict):\n",
    "    \"\"\"\n",
    "    Generate text for each row \n",
    "    Args:\n",
    "        row (pd.Series): Row of the DataFrame\n",
    "        guest_country_dict (dict): Dictionary mapping guest country codes to country names\n",
    "        month_dict (dict): Dictionary mapping month numbers to month names\n",
    "    Returns:\n",
    "        str: Text generated from the row\n",
    "    \"\"\"\n",
    "    guest_country = guest_country_dict.get(row['guest_country'], row['guest_country'])\n",
    "    month_name = month_dict.get(row['month'], str(row['month']))\n",
    "    \n",
    "    text = (\n",
    "        f\"A {row['guest_type']} from {guest_country} stayed at a {row['accommodation_type']} \"\n",
    "        f\"in {row['accommodation_country']} for {row['room_nights']} nights in {month_name}. \"\n",
    "        f\"The {row['accommodation_type']} has a score of {row['accommodation_score']} and a rating of {row['accommodation_star_rating']}.\"\n",
    "    )\n",
    "    \n",
    "    if row['location_is_ski']:\n",
    "        text += \" The location is at a ski resort.\"\n",
    "    if row['location_is_beach']:\n",
    "        text += \" The location is at the beach.\"\n",
    "    if row['location_is_city_center']:\n",
    "        text += \" The location is in the city center.\"\n",
    "    \n",
    "    return text\n",
    "\n",
    "def get_tokens(df, tokenizer, batch_size=128, max_length=512):\n",
    "    \"\"\"\n",
    "    Tokenize the text data\n",
    "    Args:\n",
    "        df (pd.DataFrame): DataFrame containing text data\n",
    "        tokenizer (transformers.PreTrainedTokenizer): Tokenizer to be used\n",
    "        batch_size (int, optional): Batch size for tokenization. Defaults to 128.\n",
    "        max_length (int, optional): Maximum length of the tokenized sequence. Defaults to 512.\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with 'input_ids' and 'attention\n",
    "    \"\"\"\n",
    "    input_ids = []\n",
    "    attention_mask = []\n",
    "    \n",
    "    for start_idx in tqdm(range(0, len(df), batch_size), desc=\"Tokenizing\", total=len(df) // batch_size):\n",
    "        end_idx = min(start_idx + batch_size, len(df))\n",
    "        batch_texts = df[\"text\"].iloc[start_idx:end_idx].tolist()\n",
    "\n",
    "        tokenized = tokenizer(\n",
    "            batch_texts,\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=max_length,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "\n",
    "        input_ids.extend(tokenized[\"input_ids\"].tolist())\n",
    "        attention_mask.extend(tokenized[\"attention_mask\"].tolist())\n",
    "\n",
    "    df[\"input_ids\"] = input_ids\n",
    "    df[\"attention_mask\"] = attention_mask\n",
    "    df = df.drop(columns=[\"text\"])\n",
    "    return df\n",
    "\n",
    "def process_users(df, embedding_model, tokenizer, scaler_accommodation_score=None, scaler_accommodation_star_rating=None):\n",
    "    \"\"\"\n",
    "    Processes user data for training, applying various transformations to the dataset.\n",
    "\n",
    "    This function processes the following aspects of the input dataframe:\n",
    "    - Text data from user reviews\n",
    "    - Guest type and country, encoding them using embeddings\n",
    "    - Room nights, seasonality, and holiday information\n",
    "    - Accommodation type and country, encoding using embeddings\n",
    "    - Scaled accommodation score and star rating\n",
    "    - Tokenizes text data using the provided tokenizer\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The dataframe containing user data to be processed.\n",
    "        embedding_model (transformers.PreTrainedModel): Pretrained embedding model used for encoding categorical data.\n",
    "        tokenizer (transformers.PreTrainedTokenizer): Tokenizer used to tokenize review text.\n",
    "        scaler_accommodation_score (sklearn.preprocessing.StandardScaler, optional): Scaler used to normalize accommodation scores. Defaults to None.\n",
    "        scaler_accommodation_star_rating (sklearn.preprocessing.StandardScaler, optional): Scaler used to normalize accommodation star ratings. Defaults to None.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing:\n",
    "            - tabular_features (pd.DataFrame): DataFrame of processed tabular features, excluding tokenization columns.\n",
    "            - tokens (pd.DataFrame): DataFrame of user-related tokens, including 'user_id', 'accommodation_id', 'input_ids', and 'attention_mask'.\n",
    "            - scaler_accommodation_score (sklearn.preprocessing.StandardScaler): The scaler used for accommodation scores, if applicable.\n",
    "            - scaler_accommodation_star_rating (sklearn.preprocessing.StandardScaler): The scaler used for accommodation star ratings, if applicable.\n",
    "    \"\"\"\n",
    "    df['text'] = df.progress_apply(get_row_text, axis=1)\n",
    "    df = process_guest_type(df)\n",
    "    df = process_guest_country(df, guest_country_dict, embedding_model)\n",
    "    df = process_room_nights(df)\n",
    "    df = process_seasons(df)\n",
    "    df = process_holidays(df)\n",
    "    df = process_month(df)\n",
    "    df = process_accommodation_type(df)\n",
    "    df = process_accommodation_country(df, embedding_model)\n",
    "    df, scaler_accommodation_score = process_accommodation_score(df)\n",
    "    df, scaler_accommodation_star_rating = process_accommodation_star_rating(df)\n",
    "    df = get_tokens(df, tokenizer)\n",
    "\n",
    "    tabular_features = df.drop(columns=['input_ids', 'attention_mask'])\n",
    "    tokens = df[['user_id', 'accommodation_id', 'input_ids', 'attention_mask']]\n",
    "    \n",
    "    return tabular_features, tokens, scaler_accommodation_score, scaler_accommodation_star_rating"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create processed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_user_features, train_user_tokens, scaler_accommodation_score, scaler_accommodation_star_rating = process_users(train_users, embedding_model, tokenizer)\n",
    "val_user_features, val_user_tokens, _, _ = process_users(val_users, embedding_model, tokenizer, scaler_accommodation_score, scaler_accommodation_star_rating)\n",
    "test_user_features, test_user_tokens, _, _ = process_users(test_users, embedding_model, tokenizer, scaler_accommodation_score, scaler_accommodation_star_rating)\n",
    "\n",
    "def equalize_columns(df1, df2):\n",
    "    \"\"\"\n",
    "    Equalize columns between two DataFrames by adding missing columns with 0 values.\n",
    "    Args:\n",
    "        df1 (pd.DataFrame): First DataFrame\n",
    "        df2 (pd.DataFrame): Second DataFrame\n",
    "    Returns:\n",
    "        tuple: A tuple containing:\n",
    "            - df1 (pd.DataFrame): First DataFrame with missing columns added\n",
    "            - df2 (pd.DataFrame): Second DataFrame with missing columns added\n",
    "    \"\"\"\n",
    "    cols1 = set(df1.columns)\n",
    "    cols2 = set(df2.columns)\n",
    "    missing_cols1 = cols2 - cols1\n",
    "    missing_cols2 = cols1 - cols2\n",
    "    for col in missing_cols1:\n",
    "        df1[col] = 0\n",
    "    for col in missing_cols2:\n",
    "        df2[col] = 0\n",
    "    return df1, df2\n",
    "\n",
    "train_user_features, val_user_features = equalize_columns(train_user_features, val_user_features)\n",
    "train_user_features, test_user_features = equalize_columns(train_user_features, test_user_features)\n",
    "val_user_features, test_user_features = equalize_columns(val_user_features, test_user_features)\n",
    "\n",
    "train_user_features_columns = train_user_features.columns\n",
    "val_features = val_user_features[train_user_features_columns]\n",
    "test_features = test_user_features[train_user_features_columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_user_features.to_parquet('train_users_features.parquet')\n",
    "train_user_tokens.to_parquet('train_users_tokens.parquet')\n",
    "\n",
    "val_features.to_parquet('val_users_features.parquet')\n",
    "val_user_tokens.to_parquet('val_users_tokens.parquet')\n",
    "\n",
    "test_features.to_parquet('test_users_features.parquet')\n",
    "test_user_tokens.to_parquet('test_users_tokens.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions for reviews processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_review_text(row):\n",
    "    \"\"\"\n",
    "    Create review text from the review data\n",
    "    Args:\n",
    "        row (pd.Series): Row of the DataFrame\n",
    "    Returns:\n",
    "        str: Review text\n",
    "    \"\"\"\n",
    "    title = str(row['review_title']) if pd.notna(row['review_title']) else ''\n",
    "    positive = str(row['review_positive']) if pd.notna(row['review_positive']) else ''\n",
    "    negative = str(row['review_negative']) if pd.notna(row['review_negative']) else ''\n",
    "    \n",
    "    return f\"Title: {title}, What I liked about my stay: {positive}, What I didn't like: {negative}\"\n",
    "\n",
    "def add_sentiment_score(df):\n",
    "    \"\"\"\n",
    "    Add sentiment scores to the review data\n",
    "    Args:\n",
    "        df (pd.DataFrame): DataFrame containing review data\n",
    "    \"\"\"\n",
    "    def get_sentiment_score(text):\n",
    "        if pd.isna(text):\n",
    "            return 0\n",
    "        return TextBlob(text).sentiment.polarity\n",
    "\n",
    "    df['title_sentiment'] = df['review_title'].progress_apply(get_sentiment_score)\n",
    "    df['positive_sentiment'] = df['review_positive'].progress_apply(get_sentiment_score)\n",
    "    df['negative_sentiment'] = df['review_negative'].progress_apply(get_sentiment_score)\n",
    "\n",
    "def add_lengths(df):\n",
    "    \"\"\"\n",
    "    Add lengths of review text to the DataFrame\n",
    "    Args:\n",
    "        df (pd.DataFrame): DataFrame containing review data\n",
    "    \"\"\"\n",
    "    df['title_length'] = df['review_title'].fillna('').str.len()\n",
    "    df['positive_length'] = df['review_positive'].fillna('').str.len()\n",
    "    df['negative_length'] = df['review_negative'].fillna('').str.len()\n",
    "\n",
    "def normalize_review_score(df, scaler=None):\n",
    "    \"\"\"\n",
    "    Normalize 'review_score' column\n",
    "    Args:\n",
    "        df (pd.DataFrame): DataFrame containing 'review_score' column\n",
    "        scaler (sklearn.preprocessing.StandardScaler, optional): Scaler to be used. Defaults to None.\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with normalized 'review_score' column\n",
    "        MinMaxScaler: Scaler used for normalization\n",
    "    \"\"\"\n",
    "    if scaler is None:\n",
    "        scaler = MinMaxScaler()\n",
    "        df['review_score'] = scaler.fit_transform(df[['review_score']])\n",
    "    else:\n",
    "        df['review_score'] = scaler.transform(df[['review_score']])\n",
    "    return df, scaler\n",
    "\n",
    "def standardize_review_helpful_votes(df, scaler=None):\n",
    "    \"\"\"\n",
    "    Standardize 'review_helpful_votes' column\n",
    "    Args:\n",
    "        df (pd.DataFrame): DataFrame containing 'review_helpful_votes' column\n",
    "        scaler (sklearn.preprocessing.StandardScaler, optional): Scaler to be used. Defaults to None.\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with standardized 'review_helpful_votes' column\n",
    "        StandardScaler: Scaler used for standardization\n",
    "    \"\"\"\n",
    "    if scaler is None:\n",
    "        scaler = StandardScaler()\n",
    "        df['review_helpful_votes'] = scaler.fit_transform(df[['review_helpful_votes']])\n",
    "    else:\n",
    "        df['review_helpful_votes'] = scaler.transform(df[['review_helpful_votes']])\n",
    "    return df, scaler\n",
    "\n",
    "def standardize_sentiments(df, title_scaler=None, positive_scaler=None, negative_scaler=None):\n",
    "    \"\"\"\n",
    "    Standardize sentiment scores\n",
    "    Args:\n",
    "        df (pd.DataFrame): DataFrame containing sentiment scores\n",
    "        title_scaler (sklearn.preprocessing.StandardScaler, optional): Scaler for title sentiment. Defaults to None.\n",
    "        positive_scaler (sklearn.preprocessing.StandardScaler, optional): Scaler for positive sentiment. Defaults to None.\n",
    "        negative_scaler (sklearn.preprocessing.StandardScaler, optional): Scaler for negative sentiment. Defaults to None.\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with standardized sentiment scores\n",
    "        StandardScaler: Scaler for title sentiment\n",
    "        StandardScaler: Scaler for positive sentiment\n",
    "        StandardScaler: Scaler for negative sentiment\n",
    "    \"\"\"\n",
    "    if title_scaler is None:\n",
    "        title_scaler = StandardScaler()\n",
    "        positive_scaler = StandardScaler()\n",
    "        negative_scaler = StandardScaler()\n",
    "        df['title_sentiment'] = title_scaler.fit_transform(df[['title_sentiment']])\n",
    "        df['positive_sentiment'] = positive_scaler.fit_transform(df[['positive_sentiment']])\n",
    "        df['negative_sentiment'] = negative_scaler.fit_transform(df[['negative_sentiment']])\n",
    "    else:\n",
    "        df['title_sentiment'] = title_scaler.transform(df[['title_sentiment']])\n",
    "        df['positive_sentiment'] = positive_scaler.transform(df[['positive_sentiment']])\n",
    "        df['negative_sentiment'] = negative_scaler.transform(df[['negative_sentiment']])\n",
    "    return df, title_scaler, positive_scaler, negative_scaler\n",
    "\n",
    "def standardize_lengths(df, title_scaler=None, positive_scaler=None, negative_scaler=None):\n",
    "    \"\"\"\n",
    "    Standardize lengths of review text\n",
    "    Args:\n",
    "        df (pd.DataFrame): DataFrame containing lengths of review text\n",
    "        title_scaler (sklearn.preprocessing.StandardScaler, optional): Scaler for title length. Defaults to None.\n",
    "        positive_scaler (sklearn.preprocessing.StandardScaler, optional): Scaler for positive length. Defaults to None.\n",
    "        negative_scaler (sklearn.preprocessing.StandardScaler, optional): Scaler for negative length. Defaults to None.\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with standardized lengths of review text\n",
    "        StandardScaler: Scaler for title length\n",
    "        StandardScaler: Scaler for positive length\n",
    "        StandardScaler: Scaler for negative length\n",
    "    \"\"\"\n",
    "    if title_scaler is None:\n",
    "        title_scaler = StandardScaler()\n",
    "        positive_scaler = StandardScaler()\n",
    "        negative_scaler = StandardScaler()\n",
    "        df['title_length'] = title_scaler.fit_transform(df[['title_length']])\n",
    "        df['positive_length'] = positive_scaler.fit_transform(df[['positive_length']])\n",
    "        df['negative_length'] = negative_scaler.fit_transform(df[['negative_length']])\n",
    "    else:\n",
    "        df['title_length'] = title_scaler.transform(df[['title_length']])\n",
    "        df['positive_length'] = positive_scaler.transform(df[['positive_length']])\n",
    "        df['negative_length'] = negative_scaler.transform(df[['negative_length']])\n",
    "    return df, title_scaler, positive_scaler, negative_scaler\n",
    "\n",
    "def add_holiday_columns(df):\n",
    "    \"\"\"\n",
    "    Add holiday columns to the DataFrame\n",
    "    Args:\n",
    "        df (pd.DataFrame): DataFrame containing review data\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with holiday columns\n",
    "    \"\"\"\n",
    "    holiday_words = {\n",
    "        \"New Year\": [\"nye\", \"new year\", \"new years\", \"new year's\", \"december 31\", \"january 1\", \"31st december\", \"1st january\", \"sylvester\"],\n",
    "        \"Valentines Day\": [\"valentine\", \"valentines\", \"cupid\", \"lovebirds\", \"proposal\"],\n",
    "        \"Easter\": [\"easter\", \"egghunt\", \"easteregg\", \"chocolateegg\", \"easterbasket\", \"resurrection\", \"goodfriday\", \"holyweek\", \"churchservice\"],\n",
    "        \"Halloween\": [\"halloween\", \"trickortreat\", \"trick or treat\"],\n",
    "        \"Christmas\": [\"christmas\", \"xmas\", \"santa claus\", \"december 25\", \"dec 25\"],\n",
    "        \"Chinese New Year\": [\"chinesenewyear\", \"lunar new\", \"chun jie\", \"liondance\", \"year of the\", \"yearofthe\", \"chinese zodiac\"],\n",
    "    }\n",
    "    for holiday, words in holiday_words.items():\n",
    "        df[holiday] = df['text'].apply(lambda x: any(word in x.lower() for word in words)).astype(int)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def is_family(df):\n",
    "    \"\"\"\n",
    "    Add 'is_family' column to the DataFrame\n",
    "    Args:\n",
    "        df (pd.DataFrame): DataFrame containing review data\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with 'is_family' column\n",
    "    \"\"\"\n",
    "    family_phrases = [\"we traveled as a family\",\"perfect for families\",\"great for families\",\"family-friendly\",\n",
    "                  \"ideal for a family trip\",\"as a family\",\"family vacation\",\"traveled with kids\",\"perfect for family stays\",\n",
    "                  \"accommodated our family perfectly\",\"great for children\",\"safe for families\",\"spacious enough for our family\",\n",
    "                  \"kid-friendly\",\"family getaway\",\"we were a family of\", \"children\", \" kids\", \"child friendly\", \"family\", \"childcare\",\n",
    "                  \"baby-friendly\", \"toddler\",  \"child amenities\", \"child safety\"\n",
    "    ]\n",
    "    def check_family(text):\n",
    "        return any(phrase.lower() in text.lower() for phrase in family_phrases)\n",
    "    \n",
    "    df['is_family'] = df['text'].apply(check_family).astype(int)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def review_to_text(row):\n",
    "    \"\"\"\n",
    "    Generate review text from the review data\n",
    "    Args:\n",
    "        row (pd.Series): Row of the DataFrame\n",
    "    Returns:\n",
    "        str: Review text\n",
    "    \"\"\"\n",
    "    review_title = row['review_title'] if pd.notna(row['review_title']) else \"Review\"\n",
    "    review_negative = row['review_negative'] if pd.notna(row['review_negative']) else \"Nothing in particular\"\n",
    "    \n",
    "    helpful_text = (\n",
    "        f\"{row['review_helpful_votes']} people found it helpful.\" \n",
    "        if row['review_helpful_votes'] > 0 \n",
    "        else \"No one found this review helpful.\"\n",
    "    )\n",
    "    \n",
    "    text = (\n",
    "        f\"{review_title}: \"\n",
    "        f\"What I liked about my stay: {row['review_positive']} \"\n",
    "        f\"What I didn't like: {review_negative} \"\n",
    "        f\"I gave a score of {row['review_score']}. \"\n",
    "        f\"{helpful_text}\"\n",
    "    )\n",
    "    return text\n",
    "\n",
    "def process_reviews(df, tokenizer, scaler_review_score=None, scaler_review_helpful_votes=None, title_length_scaler=None, positive_length_scaler=None, negative_length_scaler=None):\n",
    "    \"\"\"\n",
    "    Processes review data for training, applying various transformations to the dataset.\n",
    "    Args:\n",
    "        df (pd.DataFrame): DataFrame containing review data to be processed.\n",
    "        tokenizer (transformers.PreTrainedTokenizer): Tokenizer used to tokenize review text.\n",
    "        scaler_review_score (sklearn.preprocessing.StandardScaler, optional): Scaler used for normalizing review scores. Defaults to None.\n",
    "        scaler_review_helpful_votes (sklearn.preprocessing.StandardScaler, optional): Scaler used for standardizing review helpful votes. Defaults to None.\n",
    "        title_length_scaler (sklearn.preprocessing.StandardScaler, optional): Scaler used for standardizing title lengths. Defaults to None.\n",
    "        positive_length_scaler (sklearn.preprocessing.StandardScaler, optional): Scaler used for standardizing positive review lengths. Defaults to None.\n",
    "        negative_length_scaler (sklearn.preprocessing.StandardScaler, optional): Scaler used for standardizing negative review lengths. Defaults to None.\n",
    "    Returns:\n",
    "        tuple: A tuple containing:\n",
    "            - features (pd.DataFrame): DataFrame of processed features, excluding tokenization columns.\n",
    "            - tokens (pd.DataFrame): DataFrame of review tokens, including 'review_id', 'accommodation_id', 'input_ids', and 'attention_mask'.\n",
    "            - scaler_review_score (sklearn.preprocessing.StandardScaler): The scaler used for review scores, if applicable.\n",
    "            - scaler_review_helpful_votes (sklearn.preprocessing.StandardScaler): The scaler used for review helpful votes, if applicable.\n",
    "            - title_length_scaler (sklearn.preprocessing.StandardScaler): The scaler used for title lengths, if applicable.\n",
    "            - positive_length_scaler (sklearn.preprocessing.StandardScaler): The scaler used for positive review lengths, if applicable.\n",
    "            - negative_length_scaler (sklearn.preprocessing.StandardScaler): The scaler used for negative review lengths, if applicable.\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    df['text'] = df.progress_apply(review_to_text, axis=1)\n",
    "    add_lengths(df)\n",
    "    df.drop(['review_title', 'review_positive', 'review_negative'], axis=1, inplace=True)\n",
    "    df, scaler_review_score = normalize_review_score(df)\n",
    "    df, scaler_review_helpful_votes = standardize_review_helpful_votes(df)\n",
    "    df, title_length_scaler, positive_length_scaler, negative_length_scaler = standardize_lengths(df, title_length_scaler, positive_length_scaler, negative_length_scaler)\n",
    "    df = add_holiday_columns(df)\n",
    "    df = is_family(df)\n",
    "    df = get_tokens(df, tokenizer)\n",
    "\n",
    "    features = df.drop(columns=['input_ids', 'attention_mask'])\n",
    "    tokens = df[['review_id', 'accommodation_id', 'input_ids', 'attention_mask']]\n",
    "\n",
    "    return features, tokens, scaler_review_score, scaler_review_helpful_votes, title_length_scaler, positive_length_scaler, negative_length_scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_reviews_features, train_reviews_tokens, scaler_review_score, scaler_review_helpful_votes, title_length_scaler, positive_length_scaler, negative_length_scaler = process_reviews(train_reviews, tokenizer)\n",
    "train_reviews_features.to_parquet('train_reviews_features.parquet')\n",
    "train_reviews_tokens.to_parquet('train_reviews_tokens.parquet')\n",
    "\n",
    "val_reviews_features, val_reviews_tokens, _, _, _, _, _ = process_reviews(val_reviews, tokenizer, scaler_review_score, scaler_review_helpful_votes, title_length_scaler, positive_length_scaler, negative_length_scaler)\n",
    "val_reviews_features.to_parquet('val_reviews_features.parquet')\n",
    "val_reviews_tokens.to_parquet('val_reviews_tokens.parquet')\n",
    "\n",
    "test_reviews_features, test_reviews_tokens, _, _, _, _, _ = process_reviews(test_reviews, tokenizer, scaler_review_score, scaler_review_helpful_votes, title_length_scaler, positive_length_scaler, negative_length_scaler)\n",
    "test_reviews_features.to_parquet('test_reviews_features.parquet')\n",
    "test_reviews_tokens.to_parquet('test_reviews_tokens.parquet')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
