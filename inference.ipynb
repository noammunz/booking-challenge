{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AutoModel\n",
    "import lightgbm as lgb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "This notebook uses the LightGBM models we created as an ensemble to perform inference. It generates the necessary CSV file for Kaggle submission and saves it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_users_features = pd.read_parquet('test_users_features.parquet')\n",
    "test_reviews_features = pd.read_parquet('test_reviews_features.parquet')\n",
    "\n",
    "test_users_tokens = pd.read_parquet('test_users_tokens.parquet')\n",
    "test_reviews_tokens = pd.read_parquet('test_reviews_tokens.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions and classes used for inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_subsets(users_features, users_tokens, reviews_features, reviews_tokens, matches, total_parts, part):\n",
    "    \"\"\"\n",
    "    Create subsets of the data for training and evaluation to fit into memory\n",
    "    Args:\n",
    "        users_features: DataFrame with user features\n",
    "        users_tokens: DataFrame with user tokens\n",
    "        reviews_features: DataFrame with review features\n",
    "        reviews_tokens: DataFrame with review tokens\n",
    "        matches: DataFrame with matches\n",
    "        total_parts: total number of parts to split the data into\n",
    "        part: part to select\n",
    "    Returns:\n",
    "        Tuple of DataFrames with user features, user tokens, review features, review tokens, and matches\n",
    "    \"\"\"\n",
    "    accommodation_ids = matches['accommodation_id'].unique().tolist()\n",
    "    \n",
    "    accommodation_ids = sorted(accommodation_ids)\n",
    "    \n",
    "    subset_size = len(accommodation_ids) // total_parts\n",
    "    remainder = len(accommodation_ids) % total_parts\n",
    "    \n",
    "    start_idx = part * subset_size + min(part, remainder)\n",
    "    end_idx = (part + 1) * subset_size + min(part + 1, remainder)\n",
    "\n",
    "    selected_accommodation_ids = accommodation_ids[start_idx:end_idx]\n",
    "    \n",
    "    users_features = users_features[users_features['accommodation_id'].isin(selected_accommodation_ids)]\n",
    "    reviews_features = reviews_features[reviews_features['accommodation_id'].isin(selected_accommodation_ids)]\n",
    "    users_tokens = users_tokens[users_tokens['accommodation_id'].isin(selected_accommodation_ids)]\n",
    "    reviews_tokens = reviews_tokens[reviews_tokens['accommodation_id'].isin(selected_accommodation_ids)]\n",
    "    \n",
    "    matches_reduced = matches[matches['accommodation_id'].isin(selected_accommodation_ids)]\n",
    "    \n",
    "    return users_features, users_tokens, reviews_features, reviews_tokens, matches_reduced\n",
    "\n",
    "class TwoTowersNetwork(nn.Module):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        model_id (str): Model identifier.\n",
    "    \"\"\"\n",
    "    def __init__(self, model_id):\n",
    "        super().__init__()\n",
    "        self.bert1 = AutoModel.from_pretrained(model_id)\n",
    "        self.bert2 = AutoModel.from_pretrained(model_id)\n",
    "        \n",
    "    def mean_pooling(self, model_output, attention_mask):\n",
    "        token_embeddings = model_output[0]\n",
    "        mask = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "        return torch.sum(token_embeddings * mask, 1) / torch.clamp(mask.sum(1), min=1e-9)\n",
    "\n",
    "    def forward(self, user_ids, user_mask, review_ids, review_mask):\n",
    "        context_output = self.bert1(user_ids, attention_mask=user_mask)\n",
    "        review_output = self.bert2(review_ids, attention_mask=review_mask)\n",
    "        \n",
    "        context_embed = self.mean_pooling(context_output, user_mask)\n",
    "        review_embed = self.mean_pooling(review_output, review_mask)\n",
    "        \n",
    "        return context_embed, review_embed\n",
    "\n",
    "def pad_sequence(sequences, batch_first=False, padding_value=0):\n",
    "    \"\"\"\n",
    "    Pad a list of variable length sequences with padding_value.\n",
    "    \"\"\"\n",
    "    max_len = max(len(seq) for seq in sequences)\n",
    "    padded_sequences = []\n",
    "\n",
    "    for seq in sequences:\n",
    "        padding = torch.full((max_len - len(seq),), padding_value, dtype=seq.dtype)\n",
    "        padded_sequences.append(torch.cat((seq, padding)))\n",
    "\n",
    "    if batch_first:\n",
    "        return torch.stack(padded_sequences)\n",
    "    else:\n",
    "        return torch.stack(padded_sequences).transpose(0, 1)\n",
    "\n",
    "def encode_pairwise(train_users_tokens, train_reviews_tokens, model, batch_size=64, device='cuda'):\n",
    "    \"\"\"\n",
    "    Encode user and review tokens using the model.\n",
    "    Args:\n",
    "        train_users_tokens (DataFrame): DataFrame with user tokens\n",
    "        train_reviews_tokens (DataFrame): DataFrame with review tokens\n",
    "        model (nn.Module): Model to use for encoding\n",
    "        batch_size (int): Batch size\n",
    "        device (str): Device to use\n",
    "    Returns:\n",
    "        Tuple of DataFrames with user and review embeddings\n",
    "    \"\"\"\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    user_embeddings, review_embeddings = [], []\n",
    "\n",
    "    for i in range(0, len(train_users_tokens), batch_size):\n",
    "        user_batch = train_users_tokens.iloc[i:i+batch_size]\n",
    "        review_batch = train_reviews_tokens.iloc[i:i+batch_size]\n",
    "        \n",
    "        user_ids = pad_sequence([torch.tensor(x) for x in user_batch['input_ids']], batch_first=True).to(device)\n",
    "        user_masks = pad_sequence([torch.tensor(x) for x in user_batch['attention_mask']], batch_first=True).to(device)\n",
    "        review_ids = pad_sequence([torch.tensor(x) for x in review_batch['input_ids']], batch_first=True).to(device)\n",
    "        review_masks = pad_sequence([torch.tensor(x) for x in review_batch['attention_mask']], batch_first=True).to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            user_embed, review_embed = model(user_ids, user_masks, review_ids, review_masks)\n",
    "            user_embeddings.extend(user_embed.cpu().numpy())\n",
    "            review_embeddings.extend(review_embed.cpu().numpy())\n",
    "        \n",
    "        if (i // batch_size) % 500 == 0:\n",
    "            print(f'Processed {i}/{len(train_users_tokens)} samples', flush=True)\n",
    "    print('', flush=True)\n",
    "\n",
    "    user_embeddings_df = pd.DataFrame(user_embeddings, columns=[f'user_embedding_{i}' for i in range(user_embed.size(1))])\n",
    "    review_embeddings_df = pd.DataFrame(review_embeddings, columns=[f'review_embedding_{i}' for i in range(review_embed.size(1))])\n",
    "\n",
    "    return user_embeddings_df, review_embeddings_df\n",
    "\n",
    "def embed_data(users_tokens, reviews_tokens, model_path, batch_size=64, device='cuda'):\n",
    "    \"\"\"\n",
    "    Encode user and review tokens using the model.\n",
    "    Args:\n",
    "        users_tokens (DataFrame): DataFrame with user tokens\n",
    "        reviews_tokens (DataFrame): DataFrame with review tokens\n",
    "        model_path (str): Path to the model\n",
    "        batch_size (int): Batch size\n",
    "        device (str): Device to use\n",
    "    Returns:\n",
    "        Tuple of DataFrames with user and review embeddings\n",
    "    \"\"\"\n",
    "    model = TwoTowersNetwork(\"sentence-transformers/all-MiniLM-L12-v2\").to(device)\n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "\n",
    "    user_embeddings, review_embeddings = encode_pairwise(users_tokens, reviews_tokens, model, batch_size=batch_size, device=device)\n",
    "\n",
    "    \n",
    "    return user_embeddings, review_embeddings\n",
    "\n",
    "def generate_user_review_pairs(val_users, val_reviews, accommodation_reviews, sample_fraction=0.1):\n",
    "    \"\"\"\n",
    "    Generate user-review pairs for evaluation.\n",
    "    Args:\n",
    "        val_users (DataFrame): DataFrame with user features\n",
    "        val_reviews (DataFrame): DataFrame with review features\n",
    "        accommodation_reviews (dict): Dictionary with accommodation reviews\n",
    "        sample_fraction (float): Fraction of users to sample\n",
    "    Returns:\n",
    "        List of tuples with user ID, review ID, and combined features\n",
    "    \"\"\"\n",
    "    val_reviews_indexed = val_reviews.set_index('review_id')\n",
    "    user_review_pairs = []\n",
    "\n",
    "    for ii, user_row in val_users.iterrows():\n",
    "        user_id = user_row['user_id']\n",
    "        acc_id = user_row['accommodation_id']\n",
    "        user_features = user_row.drop(['user_id', 'accommodation_id'])\n",
    "        \n",
    "        reviews_for_acc = accommodation_reviews.get(acc_id, [])\n",
    "        \n",
    "        for review_id in reviews_for_acc:\n",
    "            if review_id in val_reviews_indexed.index:\n",
    "                review_features = val_reviews_indexed.loc[review_id].drop(['accommodation_id'])\n",
    "                \n",
    "                combined_features = pd.concat([user_features, review_features])\n",
    "                user_review_pairs.append((user_id, review_id, combined_features))\n",
    "\n",
    "        if (ii + 1) % 1000 == 0:\n",
    "            print(f'Processed {ii+1}/{len(val_users)} users', flush=True)\n",
    "\n",
    "    return user_review_pairs\n",
    "\n",
    "def predict_and_rank(user_review_pairs, model, batch_size=10000):\n",
    "    \"\"\"\n",
    "    Predict scores for user-review pairs and rank them.\n",
    "    Args:\n",
    "        user_review_pairs (list): List of tuples with user ID, review ID, and combined features\n",
    "        model (nn.Module): Model to use for prediction\n",
    "        batch_size (int): Batch size\n",
    "    Returns:\n",
    "        DataFrame with user ID, review ID, score, and rank\n",
    "    \"\"\"\n",
    "    user_ids = []\n",
    "    review_ids = []\n",
    "    features = []\n",
    "    scores = []\n",
    "\n",
    "    # Collect all pairs first\n",
    "    for user_id, review_id, combined_features in user_review_pairs:\n",
    "        user_ids.append(user_id)\n",
    "        review_ids.append(review_id)\n",
    "        features.append(combined_features.values)\n",
    "\n",
    "    # Convert features to numpy array\n",
    "    features_array = np.array(features)\n",
    "    \n",
    "    # Batch prediction\n",
    "    for i in range(0, len(features_array), batch_size):\n",
    "        batch_features = features_array[i:i + batch_size]\n",
    "        batch_scores = model.predict(batch_features)\n",
    "        scores.extend(batch_scores)\n",
    "        print(f'Predicted {i + len(batch_features)}/{len(features_array)} samples', flush=True)\n",
    "    print('', flush=True)\n",
    "\n",
    "    predictions_df = pd.DataFrame({\n",
    "        'user_id': user_ids,\n",
    "        'review_id': review_ids,\n",
    "        'score': scores\n",
    "    })\n",
    "\n",
    "    predictions_df['rank'] = predictions_df.groupby('user_id')['score'].rank(ascending=False, method='first')\n",
    "\n",
    "    return predictions_df\n",
    "\n",
    "def aggregate_review_scores(*dfs):\n",
    "    \"\"\"\n",
    "    Aggregate review scores from multiple models.\n",
    "    Args:\n",
    "        dfs: DataFrames with user ID, review ID, score, and rank\n",
    "    Returns:\n",
    "        DataFrame with user ID, review ID, aggregated score, and rank\n",
    "    \"\"\"\n",
    "    required_columns = ['user_id', 'review_id', 'score', 'rank']\n",
    "    processed_dfs = []\n",
    "    \n",
    "    for i, df in enumerate(dfs):\n",
    "        temp_df = df[required_columns].copy()\n",
    "        temp_df = temp_df.rename(columns={'score': f'score_{i+1}'})\n",
    "        processed_dfs.append(temp_df)\n",
    "    \n",
    "    final_df = processed_dfs[0]\n",
    "    for df in processed_dfs[1:]:\n",
    "        final_df = final_df.merge(df[['user_id', 'review_id', df.columns[-2]]], \n",
    "                                on=['user_id', 'review_id'], \n",
    "                                how='outer')\n",
    "    \n",
    "    score_columns = [col for col in final_df.columns if col.startswith('score_')]\n",
    "    final_df['aggregated_score'] = final_df[score_columns].mean(axis=1)\n",
    "    \n",
    "    final_df['new_rank'] = final_df.groupby('user_id')['aggregated_score'].rank(\n",
    "        ascending=False, \n",
    "        method='min'\n",
    "    )\n",
    "    \n",
    "    final_columns = ['user_id', 'review_id'] + ['aggregated_score', 'new_rank']\n",
    "    \n",
    "    final_df = final_df[final_columns].sort_values(['user_id', 'new_rank'])\n",
    "    final_df = final_df.rename(columns={'aggregated_score': 'score', 'new_rank': 'rank'})\n",
    "    \n",
    "    return final_df\n",
    "\n",
    "def create_top_10_reviews_df(predictions_df, user_to_accommodation):\n",
    "    \"\"\"\n",
    "    Create DataFrame with top 10 reviews for each user.\n",
    "    Args:\n",
    "        predictions_df (DataFrame): DataFrame with user ID, review ID, score, and rank\n",
    "        user_to_accommodation (dict): Dictionary with user to accommodation mapping\n",
    "    Returns:\n",
    "        DataFrame with user ID, accommodation ID, and top 10 reviews\n",
    "    \"\"\"\n",
    "    top_10_df = predictions_df[predictions_df['rank'] <= 10].sort_values(['user_id', 'rank'])\n",
    "    \n",
    "    user_reviews_dict = {\n",
    "        user_id: group['review_id'].tolist() \n",
    "        for user_id, group in top_10_df.groupby('user_id')\n",
    "    }\n",
    "    \n",
    "    result_rows = []\n",
    "    for user_id in user_reviews_dict:\n",
    "        user_reviews = user_reviews_dict[user_id]\n",
    "        \n",
    "        user_reviews.extend([None] * (10 - len(user_reviews)))\n",
    "        \n",
    "        row = {\n",
    "            'accommodation_id': user_to_accommodation.get(user_id),\n",
    "            'user_id': user_id,\n",
    "        }\n",
    "        \n",
    "        for i, review in enumerate(user_reviews, 1):\n",
    "            row[f'review_{i}'] = review\n",
    "            \n",
    "        result_rows.append(row)\n",
    "    \n",
    "    result_df = pd.DataFrame(result_rows)\n",
    "    \n",
    "    return result_df\n",
    "\n",
    "def get_partition(accommodation_ids, index, total_partitions=5):\n",
    "    \"\"\"\n",
    "    Get partition of accommodation IDs.\n",
    "    Args:\n",
    "        accommodation_ids (list): List of accommodation IDs\n",
    "        index (int): Index of the partition\n",
    "        total_partitions (int): Total number of partitions\n",
    "    Returns:\n",
    "        List of accommodation IDs\n",
    "    \"\"\"\n",
    "    partition_size = len(accommodation_ids) // total_partitions\n",
    "    remainder = len(accommodation_ids) % total_partitions\n",
    "    \n",
    "    print(f\"Part {index} out of {total_partitions - 1}\\n\", flush=True)\n",
    "    print(f\"Partition size: {partition_size}\", flush=True)\n",
    "    print(f\"Remainder: {remainder}\", flush=True)\n",
    "\n",
    "    start_idx = index * partition_size + min(index, remainder)\n",
    "    end_idx = start_idx + partition_size + (1 if index < remainder else 0)\n",
    "\n",
    "    print(f\"Start index: {start_idx}\", flush=True)\n",
    "    print(f\"End index: {end_idx}\\n\", flush=True)\n",
    "    \n",
    "    return accommodation_ids[start_idx:end_idx]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get dataset parition so we can fit test set in memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_parts = 200\n",
    "part = 0\n",
    "\n",
    "accommodation_ids = test_users_features['accommodation_id'].unique().tolist()\n",
    "accommodation_ids = get_partition(accommodation_ids, part, total_parts)\n",
    "\n",
    "test_users_features = test_users_features[test_users_features['accommodation_id'].isin(accommodation_ids)]\n",
    "test_reviews_features = test_reviews_features[test_reviews_features['accommodation_id'].isin(accommodation_ids)]\n",
    "test_users_tokens = test_users_tokens[test_users_tokens['accommodation_id'].isin(accommodation_ids)]\n",
    "test_reviews_tokens = test_reviews_tokens[test_reviews_tokens['accommodation_id'].isin(accommodation_ids)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### embed the the test set with our trained embedding model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = 'embedding_model.pt'\n",
    "test_user_embeddings, test_review_embeddings = embed_data(test_users_tokens, test_reviews_tokens, model_path)\n",
    "\n",
    "test_users_features.reset_index(drop=True, inplace=True)\n",
    "test_reviews_features.reset_index(drop=True, inplace=True)\n",
    "test_user_embeddings.reset_index(drop=True, inplace=True)\n",
    "test_review_embeddings.reset_index(drop=True, inplace=True)\n",
    "\n",
    "\n",
    "test_users = pd.concat([test_users_features, test_user_embeddings], axis=1)\n",
    "test_reviews = pd.concat([test_reviews_features, test_review_embeddings], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### generate a pair of user-review for every user with every review for their respective accommodation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accommodation_reviews = (test_reviews.groupby('accommodation_id')['review_id'].apply(list).to_dict())\n",
    "user_review_pairs = generate_user_review_pairs(test_users, test_reviews, accommodation_reviews)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### loads our ensemble and predicts top 10 reviews for each user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_paths = ['model1.txt', 'model2.txt', 'model3.txt', 'model4.txt', 'model5.txt']\n",
    "models = [lgb.Booster(model_file=path) for path in models_paths]\n",
    "predictions_dfs = [predict_and_rank(user_review_pairs, model) for model in models]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aggregates results by getting mean score for each user-review pair and ranking reviews based on mean score. Then saves aggregated results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregate_df = aggregate_review_scores(*predictions_dfs)\n",
    "result_df = create_top_10_reviews_df(aggregate_df, test_users_features.set_index('user_id')['accommodation_id'].to_dict())\n",
    "result_df.to_csv(f'results_{part}', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### After inferencing the entire test set, we need to combine all the results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
